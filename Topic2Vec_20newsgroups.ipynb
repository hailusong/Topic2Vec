{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC2VEC algorithm by using gensim and according to the second hint given by Gordon Mohr.  \n",
    "(https://groups.google.com/forum/#!topic/gensim/BVu5-pD6910)\n",
    "\n",
    "\n",
    "1. Vectorization of docs by using CountVectorizer (with or without tfidf) with no lemmatization\n",
    "2. Latent Dirichlet Allocation \n",
    "3. Topic2Vec of the entire dataset (20 NewsGroups)   \n",
    "\n",
    "It saves:\n",
    "* the topic2vec model obtained\n",
    "\n",
    "Changes:\n",
    "1. Minor changes to be compatible with newer gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyorient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import codecs \n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import pyorient\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time, sleep\n",
    "import string\n",
    "import re\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING DOCS FROM 20 NEWSGROUPS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['comp.sys.ibm.pc.hardware',\n",
    "'comp.sys.mac.hardware',\n",
    "'comp.windows.x',\n",
    "'rec.sport.baseball',\n",
    "'rec.sport.hockey',\n",
    "'sci.med',\n",
    "'sci.space',\n",
    "'soc.religion.christian']\n",
    "\n",
    "n_topics = len(categories)\n",
    "\n",
    "categories_source = {}\n",
    "\n",
    "for cat in categories:\n",
    "    categories_source[cat] = cat.replace('.', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.sys.ibm.pc.hardware': 'comp_sys_ibm_pc_hardware',\n",
       " 'comp.sys.mac.hardware': 'comp_sys_mac_hardware',\n",
       " 'comp.windows.x': 'comp_windows_x',\n",
       " 'rec.sport.baseball': 'rec_sport_baseball',\n",
       " 'rec.sport.hockey': 'rec_sport_hockey',\n",
       " 'sci.med': 'sci_med',\n",
       " 'sci.space': 'sci_space',\n",
       " 'soc.religion.christian': 'soc_religion_christian'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space sci_space\n",
      "sci.med sci_med\n",
      "soc.religion.christian soc_religion_christian\n",
      "comp.sys.ibm.pc.hardware comp_sys_ibm_pc_hardware\n",
      "comp.sys.mac.hardware comp_sys_mac_hardware\n",
      "rec.sport.hockey rec_sport_hockey\n",
      "rec.sport.baseball rec_sport_baseball\n",
      "comp.windows.x comp_windows_x\n"
     ]
    }
   ],
   "source": [
    "for i,j in categories_source.items():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOTAL NUMBER OF DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4744"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs = newsgroups_train.filenames.shape[0]\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/sci.space/61065',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/52618',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/comp.windows.x/67032',\n",
       "       ...,\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/52576',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/soc.religion.christian/20809',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/soc.religion.christian/20733'],\n",
       "      dtype='<U96')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA to find the topic most-associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once before doing Lemmatization\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the casting of the original tag in a single\n",
    "# character which is accepted by the lemmatizer\n",
    "import nltk.corpus  # splits on punctuactions   \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import re\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    # I recognize the initial character of the word, identifying the type\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from nltk import word_tokenize, pos_tag        \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenized_doc = word_tokenize(doc) # splits on punctuactions  \n",
    "        tagged_doc = pos_tag(tokenized_doc)\n",
    "        \n",
    "        lemmatized_doc = []\n",
    "        # Scan the (word, tag) tuples which are the elements of tagged_tweet1\n",
    "        for word, tag in tagged_doc:\n",
    "            ret_value = get_wordnet_pos(tag)\n",
    "            # If the function does not return None I provide the ret_value\n",
    "            if ret_value != None:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word, get_wordnet_pos(tag)))\n",
    "            # If the function returns None I do not provide the ret_value\n",
    "            else:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word))\n",
    "        nonumbers_nopunct_lemmatized_doc = [word for word in lemmatized_doc if re.search('[a-zA-Z]{2,}', word)]\n",
    "#        nonumbers_nopunct_lemmatized_doc = [word for word in nopunct_lemmatized_doc if not re.search('\\d+', word)]\n",
    "        lemmatized_doc_stopw = [word for word in nonumbers_nopunct_lemmatized_doc if word not in stop_words]\n",
    "        \n",
    "        return lemmatized_doc_stopw #[self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit vectorizer with lemmatization done in 86.246s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='utf-8', analyzer='word',\n",
    "                                stop_words=[\"'s\", \"fx\"], ngram_range = (1,1), min_df = 2).fit(newsgroups_train.data)\n",
    "print(\"fit vectorizer with lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=[\"'s\", 'fx'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<__main__.LemmaTokenizer object at 0x7f2eab3324e0>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit vectorizer without lemmatization done in 0.805s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(encoding='utf-8', analyzer='word', stop_words='english',\n",
    "                                ngram_range = (1,1), min_df = 2, token_pattern = '[a-zA-Z]{2,}').fit(newsgroups_train.data)\n",
    "print(\"fit vectorizer without lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='[a-zA-Z]{2,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_feature_names() returns vocabulary\n",
    "n_features = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17197"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'.sh\",\n",
       "  \"'5n86qt97\",\n",
       "  \"'advanced\",\n",
       "  \"'all\",\n",
       "  \"'anal\",\n",
       "  \"'anon\",\n",
       "  \"'any\",\n",
       "  \"'best\",\n",
       "  \"'bout\",\n",
       "  \"'build\"],\n",
       " ['zterm',\n",
       "  'zubov',\n",
       "  'zupancic',\n",
       "  'zupcic',\n",
       "  'zyxel',\n",
       "  '~/app-defaults',\n",
       "  '~/pub/ioccc',\n",
       "  '~1.5mb/s',\n",
       "  '~30mb/s',\n",
       "  '~5mb/s']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf_vectorizer.get_feature_names()[:10], tf_vectorizer.get_feature_names()[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hmmm. I seem to recall that the attraction of solid state record-\\nplayers and radios in the 1960s wasn't better performance but lower\\nper-unit cost than vacuum-tube systems.\\n\\n\\tMind you, my father was a vacuum-tube fan in the 60s (Switched\\nto solid-state in the mid-seventies and then abruptly died; no doubt\\nthere's a lesson in that) and his account could have been biased.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_docs = tf_vectorizer.transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4744x17197 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 265561 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf_vectorizer = TfidfTransformer(sublinear_tf=False, use_idf = True).fit(tf_docs)\n",
    "tfidf_docs = tfidf_vectorizer.transform(tf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.decomposition.online_lda.LatentDirichletAllocation"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=4744 and n_features=17197...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 12.015s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_docs, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf_docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "hit ball cub hitter pitch catcher dodger field pitcher runner base run suck bat dept catch cubs swing water home\n",
      "Topic #1:\n",
      "get use one would like know drive think make time problem work good could see also go much need system\n",
      "Topic #2:\n",
      "gm van chi bos de det tor nyr que nyi baltimore wsh moncton edm pit min nj phi mtl rochester\n",
      "Topic #3:\n",
      "vote ron seizure jb gr david michael corn john cereal voting dale yes netcom.com unmoderated diet ballot cone sugar placebo\n",
      "Topic #4:\n",
      "god say one would people believe think know jesus christian make time church come many also go take thing life\n",
      "Topic #5:\n",
      "space file program use entry include available information system launch also source nasa list output data center satellite orbit send\n",
      "Topic #6:\n",
      "window widget use server application display client manager run event resource motif call set xterm code value xlib problem error\n",
      "Topic #7:\n",
      "game team year win play player go season last first pt score get goal second league good period hockey right\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf_feature_names is vocabulary\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 17197)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row# of lda.components_ is topic#\n",
    "# columns are the topic's unnormalized word weights (for all vocabulary words), in the sequence of the vocabulary word order\n",
    "per_topic_distr_LDA = lda.components_\n",
    "per_topic_distr_LDA.shape\n",
    "#per_topic_distr_LDA.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TOPIC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax example:\n",
    "# >>> kkk\n",
    "# array([[1, 2, 3],\n",
    "#       [0, 4, 2]])\n",
    "# >>> np.argmax(kkk, 0)\n",
    "# array([0, 1, 0])\n",
    "# >>> np.argmax(kkk, 1)\n",
    "# array([2, 1])\n",
    "#\n",
    "# this will select the topic with the most word weight for each word in the vocabulary\n",
    "# after this, we can easily lookup the best topic of each vocabulary word by \n",
    "# most_p_topic[word_voca_index] -> word's topic (0 -7 in this case) \n",
    "most_p_topic = np.argmax(per_topic_distr_LDA, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_topic_distr_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17197,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_p_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_and_topic = zip(tf_feature_names, most_p_topic)\n",
    "# word2topic_dict = {word : 'topic_' + np.array_str(topic) for word, topic in word_and_topic}\n",
    "word2topic_dict = {word : 'topic_{}'.format(topic) for word, topic in word_and_topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disrespectful', 'topic_4'),\n",
       " ('closed', 'topic_4'),\n",
       " ('fraud', 'topic_5'),\n",
       " ('ventura', 'topic_7'),\n",
       " ('powerful', 'topic_1')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the top 5 words and their belonged topics\n",
    "from itertools import islice\n",
    "list(islice(word2topic_dict.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    text_list = text.split()\n",
    "    normalized_text = [x.lower() for x in text_list]\n",
    "    # Define an empty list\n",
    "    nostopwords_text = []\n",
    "    # Scan the words\n",
    "    for word in normalized_text:\n",
    "        # Determine if the word is contained in the stop words list\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            # If the word is not contained I append it\n",
    "            nostopwords_text.append(word)\n",
    "    tokenized_text = [word for word in nostopwords_text if re.search('[a-zA-Z]{2,}', word)]\n",
    "            \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_doc_to_topic(tokenized_text, prefix, doc_id_number, word2topic_dict):\n",
    "    doc_to_topic_list = [prefix + '_' + str(doc_id_number)]\n",
    "    # print('adding doc_to_topic header element {}'.format(doc_to_topic_list[0]))\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in word2topic_dict.keys():\n",
    "            doc_to_topic_list.append(word2topic_dict[word])\n",
    "        # else:\n",
    "        #    print('{} not found in word2topic_dict.keys'.format(word))\n",
    "\n",
    "    return doc_to_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.deprecated.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, docs_list, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.docs_list = docs_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            words_doc=tokenizer(doc)\n",
    "            tags_doc = map_doc_to_topic(words_doc, idx, word2topic_dict)\n",
    "            yield LabeledSentence(words = words_doc,\n",
    "                                                 tags = tags_doc)\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(models.doc2vec.LabeledSentence)\n",
    "        return models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence_training(object):\n",
    "    def __init__(self, sources, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.sources = sources\n",
    "        flipped = {}\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        print('len of sources is {}'.format(len(self.sources)))\n",
    "        for source, prefix in self.sources.items():\n",
    "            print(source)\n",
    "            newsgroups_train_cat = fetch_20newsgroups(subset='train',\n",
    "                                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=[source])\n",
    "            # print('len of newsgroups_train_cat is {}'.format(len(newsgroups_train_cat)))\n",
    "            # (Pdb) newsgroups_train_cat.keys() -> \n",
    "            # dict_keys(['data', 'filenames', 'target', 'description', 'DESCR', 'target_names'])\n",
    "            # import pdb; pdb.set_trace()\n",
    "            for idx, doc in enumerate(newsgroups_train_cat.data):\n",
    "                words_doc=tokenizer(doc)\n",
    "                tags_doc = map_doc_to_topic(words_doc, prefix, idx, word2topic_dict)\n",
    "                yield LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc)\n",
    "                \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        print('len of sources is {}'.format(len(self.sources)))\n",
    "        for source, prefix in self.sources.items():\n",
    "            newsgroups_train_cat = fetch_20newsgroups(subset='train',\n",
    "                                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=[source])\n",
    "            # print('len of newsgroups_train_cat is {}'.format(len(newsgroups_train_cat)))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # (Pdb) type(newsgroups_train_cat) -> <class 'sklearn.utils.Bunch'> => len is 6\n",
    "            # (Pdb) type(newsgroups_train_cat.data) -> <class 'list'>\n",
    "            # (Pdb) len(newsgroups_train_cat.data) -> 593\n",
    "            # (Pdb) newsgroups_train_cat.data[0] -> document 1 strings, with newlines inside\n",
    "            # (Pdb) newsgroups_train_cat.data[1] -> document 2 strings, with newlines inside\n",
    "            # (Pdb) newsgroups_train_cat.target.shape -> (593,)\n",
    "            # (Pdb) newsgroups_train_cat.target.max() -> 0\n",
    "            for idx, doc in enumerate(newsgroups_train_cat.data):\n",
    "                words_doc=tokenizer(doc)\n",
    "                tags_doc = map_doc_to_topic(words_doc, prefix, idx, word2topic_dict)\n",
    "                self.sentences.append(LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc))\n",
    "        return self.sentences\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.sys.ibm.pc.hardware': 'comp_sys_ibm_pc_hardware',\n",
       " 'comp.sys.mac.hardware': 'comp_sys_mac_hardware',\n",
       " 'comp.windows.x': 'comp_windows_x',\n",
       " 'rec.sport.baseball': 'rec_sport_baseball',\n",
       " 'rec.sport.hockey': 'rec_sport_hockey',\n",
       " 'sci.med': 'sci_med',\n",
       " 'sci.space': 'sci_space',\n",
       " 'soc.religion.christian': 'soc_religion_christian'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disrespectful', 'topic_4'),\n",
       " ('closed', 'topic_4'),\n",
       " ('fraud', 'topic_5'),\n",
       " ('ventura', 'topic_7'),\n",
       " ('powerful', 'topic_1')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(word2topic_dict.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all input news group documents\n",
    "#    For all sentences in that document\n",
    "#        Generate gensim.models.deprecated.doc2vec.LabeledSentence\n",
    "#            (words, tags with group name and word's topics)\n",
    "it = LabeledLineSentence_training(categories_source, word2topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quote notes about LabeledSentence and TaggedDocument\n",
    "1. LabeledSentence is an older, deprecated name for the same simple object-type to encapsulate a text-example that is now called TaggedDocument. \n",
    "2. Any objects that have words and tags properties, each a list, will do.\n",
    "    - words is always a list of strings\n",
    "    - tags can be a mix of integers and strings, but in the common and most-efficient case, is just a list with a single id integer, starting at 0.)\n",
    "\n",
    "#### [Info about how to use Gensim doc2vec](https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104)\n",
    "1. In this example it uses filename and doc label\n",
    "2. And after the training it can print the vector of the file using its name\n",
    "\n",
    "    ```\n",
    "    docvec = d2v_model.docvecs[‘1.txt’] #if string tag used in training\n",
    "    print docvec\n",
    "    ```\n",
    "3. Or to get most similar document with similarity scores using document-index\n",
    "\n",
    "    ```\n",
    "    similar_doc = d2v_model.docvecs.most_similar(14) \n",
    "    print similar_doc\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sources is 8\n",
      "sci.space\n",
      "<class 'gensim.models.deprecated.doc2vec.LabeledSentence'>\n",
      "LabeledSentence(['lunar', 'satellite', 'needs', 'fuel', 'regular', 'orbit', 'corrections', 'fuel', 'runs', 'crash', 'months', 'orbits', 'apollo', 'motherships', 'changed', 'noticeably', 'lunar', 'missions', 'lasting', 'days', 'possible', 'stable', 'orbits', 'moons', 'gravitational', 'field', 'poorly', 'mapped', 'know', 'perturbations', 'sun', 'earth', 'relatively', 'minor', 'issues', 'low', 'altitudes', 'big', 'problem', 'moons', 'gravitational', 'field', 'quite', 'lumpy', 'irregular', 'distribution', 'mass', 'moon'], ['sci_space_0', 'topic_5', 'topic_5', 'topic_5', 'topic_7', 'topic_5', 'topic_5', 'topic_1', 'topic_5', 'topic_5', 'topic_1', 'topic_1', 'topic_5', 'topic_5', 'topic_1', 'topic_1', 'topic_5', 'topic_5', 'topic_1', 'topic_7', 'topic_1', 'topic_1', 'topic_1', 'topic_5', 'topic_5', 'topic_1', 'topic_5', 'topic_5', 'topic_5', 'topic_5'])\n",
      "30 48\n",
      "['sci_space_0', 'topic_5', 'topic_5', 'topic_5', 'topic_7', 'topic_5', 'topic_5', 'topic_1', 'topic_5', 'topic_5'] ['lunar', 'satellite', 'needs', 'fuel', 'regular', 'orbit', 'corrections', 'fuel', 'runs', 'crash']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "# print the first new group's item #1\n",
    "inspect_item = next(iter(it))\n",
    "print(type(inspect_item))\n",
    "print(inspect_item)\n",
    "print(len(inspect_item.tags), len(inspect_item.words))\n",
    "print(inspect_item.tags[:10], inspect_item.words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/gensim/models/doc2vec.py:359: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sources is 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "# type(models.Doc2Vec)\n",
    "model = models.Doc2Vec(size=100, window=10, min_count=4, dm=1, dbow_words=1,\n",
    "                              workers=50, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "model.build_vocab(it.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:39<00:00, 13.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(20)):\n",
    "    model.train(it.sentences_perm(), total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname =  os.getcwd() # Prints the working directory\n",
    "fname = fname + '/topic2vec_20NG_2_ndoc' + str(n_docs) + 'n_topic' + str(n_topics) + '.model'\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results\n",
    "A quick info about [how to use gensim doc2vec model to query words by label vector or vice versa](https://github.com/RaRe-Technologies/gensim/issues/1397)\n",
    "\n",
    "1. search words using word\n",
    "\n",
    "    ```\n",
    "    model.most_similar('word')\n",
    "    # only similar words were returned but not labels\n",
    "    ```\n",
    "2. search label by label\n",
    "    - use model.docvecs.most_similar to search for similar labels using labels\n",
    "3. search words by label\n",
    "\n",
    "    ```\n",
    "    model.docvecs['label']\n",
    "    model.similar_by_vector(label_vec)\n",
    "    # only similar words were returned\n",
    "    ```\n",
    "4. search labels by word\n",
    "\n",
    "    ```\n",
    "    word_vec = model['word']\n",
    "    model.docvecs.most_similar([word_vec])\n",
    "    # returns similar labels\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from /home/aimladmin/notebooks/home/ksong/Topic2Vec/topic2vec_20NG_2_ndoc4744n_topic8.model\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# load the model back\n",
    "fname = fname if fname is not None else 'topic2vec_20NG_2_ndoc4744n_topic8.model'\n",
    "print('loading model from {}'.format(fname))\n",
    "d2v_model = models.doc2vec.Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4752 [('comp_sys_ibm_pc_hardware_354', Doctag(offset=2148, word_count=224, doc_count=1)), ('comp_sys_ibm_pc_hardware_326', Doctag(offset=2120, word_count=13, doc_count=1)), ('comp_windows_x_442', Doctag(offset=4601, word_count=72, doc_count=1)), ('soc_religion_christian_326', Doctag(offset=1521, word_count=10, doc_count=1)), ('comp_sys_ibm_pc_hardware_291', Doctag(offset=2085, word_count=58, doc_count=1))]\n"
     ]
    }
   ],
   "source": [
    "# list the top 5 tags in the model\n",
    "from itertools import islice\n",
    "\n",
    "paragraphs_tag = d2v_model.docvecs.doctags\n",
    "type(paragraphs_tag)\n",
    "print(len(paragraphs_tag), list(islice(paragraphs_tag.items(),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4752, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragraphs_vector = d2v_model.docvecs.doctag_syn0\n",
    "ragraphs_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rec_sport_baseball_389', 0.35365036129951477),\n",
       " ('rec_sport_baseball_378', 0.34690025448799133),\n",
       " ('comp_sys_ibm_pc_hardware_563', 0.3393045663833618),\n",
       " ('sci_med_524', 0.3304837942123413),\n",
       " ('comp_sys_mac_hardware_518', 0.32284557819366455),\n",
       " ('sci_med_125', 0.3165658116340637),\n",
       " ('rec_sport_hockey_94', 0.28935739398002625),\n",
       " ('comp_sys_mac_hardware_292', 0.289227157831192),\n",
       " ('rec_sport_hockey_203', 0.285552978515625),\n",
       " ('rec_sport_baseball_449', 0.28499162197113037)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.most_similar(positive = ['sci_space_96'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lords', 0.39834630489349365),\n",
       " ('doesnt', 0.3908179998397827),\n",
       " ('darling', 0.3805291950702667),\n",
       " ('ahola', 0.35309669375419617),\n",
       " ('worry', 0.3530368208885193),\n",
       " ('destroyed', 0.3495197892189026),\n",
       " ('tale', 0.33816206455230713),\n",
       " ('cpus', 0.33736613392829895),\n",
       " ('lzone', 0.325408935546875),\n",
       " ('ears', 0.3231354355812073)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vec = d2v_model.docvecs['sci_space_96']\n",
    "d2v_model.wv.similar_by_vector(label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> top 10 relevant words of topic 0\n",
      "[('al', 0.8068424463272095), ('holds', 0.7901829481124878), ('rd3', 0.7826679944992065), ('percentage', 0.7784439921379089), ('behalf', 0.7727761268615723), ('spouse', 0.7723900079727173), ('intensive', 0.7632749080657959), ('crime', 0.7625982165336609), ('molecular', 0.7587553858757019), ('experimental', 0.7542406320571899)]\n",
      ">>> top 10 relevant words of topic 1\n",
      "[('echohostname', 0.922170877456665), ('hank', 0.9113080501556396), ('echo', 0.9076107144355774), ('set', 0.9072037935256958), ('woof', 0.8928797841072083), ('aaron', 0.8849927186965942), ('tail', 0.8846719264984131), ('iivx', 0.8805248141288757), ('finished', 0.8791848421096802), ('cdrom', 0.8782916069030762)]\n",
      ">>> top 10 relevant words of topic 2\n",
      "[('decs', 0.8631792068481445), ('create', 0.8595727682113647), ('exposuremask', 0.8494325876235962), ('waking', 0.8476630449295044), ('spoke', 0.8470355272293091), ('event', 0.8446255922317505), ('program', 0.8435776829719543), ('meditating', 0.8421506881713867), ('restored', 0.8416558504104614), ('icon', 0.8404950499534607)]\n",
      ">>> top 10 relevant words of topic 3\n",
      "[('crap', 0.7798736095428467), ('dram', 0.7096757292747498), ('fluid', 0.6836119890213013), ('cycle', 0.6735010147094727), ('someones', 0.6624869108200073), ('falling', 0.6527544856071472), ('affect', 0.6497668623924255), ('propulsion', 0.6358532309532166), ('minors', 0.6320293545722961), ('single', 0.6275367736816406)]\n",
      ">>> top 10 relevant words of topic 4\n",
      "[('dreams', 0.7711558938026428), ('mu', 0.7572609186172485), ('mean', 0.7414233684539795), ('connection', 0.7377687692642212), ('judge', 0.7367845177650452), ('explosions', 0.7366541624069214), ('ideas', 0.736388623714447), ('choosing', 0.7312091588973999), ('believed', 0.729036808013916), ('reformat', 0.7234945297241211)]\n",
      ">>> top 10 relevant words of topic 5\n",
      "[('icon', 0.8683058023452759), ('set', 0.8650736808776855), ('mechanism', 0.8590324521064758), ('ms', 0.8565715551376343), ('coordinates', 0.8454750776290894), ('blit', 0.8400388360023499), ('save', 0.8353490829467773), ('xclrs', 0.834591269493103), ('en', 0.8300473690032959), ('woof', 0.8297919034957886)]\n",
      ">>> top 10 relevant words of topic 6\n",
      "[('health', 0.845354437828064), ('snuff', 0.7913036346435547), ('tobacco', 0.7913007736206055), ('pray', 0.783969521522522), ('brother', 0.7774941921234131), ('punishment', 0.776374340057373), ('chewing', 0.7514508962631226), ('sisterinlaw', 0.7512537837028503), ('yankees', 0.7459442019462585), ('smokeless', 0.7427297830581665)]\n",
      ">>> top 10 relevant words of topic 7\n",
      "[('receive', 0.8917831778526306), ('warned', 0.8816089630126953), ('expose', 0.8808605074882507), ('drawable', 0.868147611618042), ('mainwinwin', 0.8607932925224304), ('drawing', 0.846565842628479), ('excerpts', 0.832735538482666), ('detailwinwin', 0.8278787136077881), ('decs', 0.827242374420166), ('receives', 0.8075612783432007)]\n"
     ]
    }
   ],
   "source": [
    "for topic_idx in range(8):\n",
    "    print('>>> top 10 relevant words of topic {}'.format(topic_idx))\n",
    "    topic_vec = d2v_model.docvecs['topic_{}'.format(topic_idx)]\n",
    "    print(d2v_model.wv.similar_by_vector(topic_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sci_space_411', 0.6604948043823242),\n",
       " ('comp_sys_mac_hardware_42', 0.6437797546386719),\n",
       " ('soc_religion_christian_278', 0.6072692275047302),\n",
       " ('rec_sport_baseball_549', 0.5811659097671509),\n",
       " ('comp_windows_x_404', 0.5668190717697144),\n",
       " ('rec_sport_hockey_233', 0.5499863028526306),\n",
       " ('soc_religion_christian_28', 0.5372079610824585),\n",
       " ('comp_sys_mac_hardware_476', 0.47990018129348755),\n",
       " ('comp_sys_mac_hardware_125', 0.47634610533714294),\n",
       " ('soc_religion_christian_226', 0.4659426808357239)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec = d2v_model['nasa']\n",
    "d2v_model.docvecs.most_similar([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644150725372538"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.n_similarity(['topic_0', 'topic_2'], ['topic_3', 'topic_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3563553612509076"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.similarity('topic_0', 'topic_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
