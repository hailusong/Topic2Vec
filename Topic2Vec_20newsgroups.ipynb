{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC2VEC algorithm by using gensim and according to the second hint given by Gordon Mohr.  \n",
    "(https://groups.google.com/forum/#!topic/gensim/BVu5-pD6910)\n",
    "\n",
    "\n",
    "1. Vectorization of docs by using CountVectorizer (with or without tfidf) with no lemmatization\n",
    "2. Latent Dirichlet Allocation \n",
    "3. Topic2Vec of the entire dataset (20 NewsGroups)   \n",
    "\n",
    "It saves:\n",
    "* the topic2vec model obtained\n",
    "\n",
    "Changes:\n",
    "1. Minor changes to be compatible with newer gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyorient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import codecs \n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import pyorient\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time, sleep\n",
    "import string\n",
    "import re\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING DOCS FROM 20 NEWSGROUPS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['comp.sys.ibm.pc.hardware',\n",
    "'comp.sys.mac.hardware',\n",
    "'comp.windows.x',\n",
    "'rec.sport.baseball',\n",
    "'rec.sport.hockey',\n",
    "'sci.med',\n",
    "'sci.space',\n",
    "'soc.religion.christian']\n",
    "\n",
    "n_topics = len(categories)\n",
    "\n",
    "categories_source = {}\n",
    "\n",
    "for cat in categories:\n",
    "    categories_source[cat] = cat.replace('.', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.sys.ibm.pc.hardware': 'comp_sys_ibm_pc_hardware',\n",
       " 'comp.sys.mac.hardware': 'comp_sys_mac_hardware',\n",
       " 'comp.windows.x': 'comp_windows_x',\n",
       " 'rec.sport.baseball': 'rec_sport_baseball',\n",
       " 'rec.sport.hockey': 'rec_sport_hockey',\n",
       " 'sci.med': 'sci_med',\n",
       " 'sci.space': 'sci_space',\n",
       " 'soc.religion.christian': 'soc_religion_christian'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey rec_sport_hockey\n",
      "comp.windows.x comp_windows_x\n",
      "soc.religion.christian soc_religion_christian\n",
      "sci.space sci_space\n",
      "rec.sport.baseball rec_sport_baseball\n",
      "comp.sys.ibm.pc.hardware comp_sys_ibm_pc_hardware\n",
      "sci.med sci_med\n",
      "comp.sys.mac.hardware comp_sys_mac_hardware\n"
     ]
    }
   ],
   "source": [
    "for i,j in categories_source.items():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOTAL NUMBER OF DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4744"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs = newsgroups_train.filenames.shape[0]\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/sci.space/61065',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/52618',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/comp.windows.x/67032',\n",
       "       ...,\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.hockey/52576',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/soc.religion.christian/20809',\n",
       "       '/home/aimladmin/scikit_learn_data/20news_home/20news-bydate-train/soc.religion.christian/20733'],\n",
       "      dtype='<U96')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA to find the topic most-associated with each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH Lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Return the casting of the original tag in a single\n",
    "# character which is accepted by the lemmatizer\n",
    "import nltk.corpus  # splits on punctuactions   \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import re\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    # I recognize the initial character of the word, identifying the type\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from nltk import word_tokenize, pos_tag        \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenized_doc = word_tokenize(doc) # splits on punctuactions  \n",
    "        tagged_doc = pos_tag(tokenized_doc)\n",
    "        \n",
    "        lemmatized_doc = []\n",
    "        # Scan the (word, tag) tuples which are the elements of tagged_tweet1\n",
    "        for word, tag in tagged_doc:\n",
    "            ret_value = get_wordnet_pos(tag)\n",
    "            # If the function does not return None I provide the ret_value\n",
    "            if ret_value != None:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word, get_wordnet_pos(tag)))\n",
    "            # If the function returns None I do not provide the ret_value\n",
    "            else:\n",
    "                lemmatized_doc.append(self.wnl.lemmatize(word))\n",
    "        nonumbers_nopunct_lemmatized_doc = [word for word in lemmatized_doc if re.search('[a-zA-Z]{2,}', word)]\n",
    "#        nonumbers_nopunct_lemmatized_doc = [word for word in nopunct_lemmatized_doc if not re.search('\\d+', word)]\n",
    "        lemmatized_doc_stopw = [word for word in nonumbers_nopunct_lemmatized_doc if word not in stop_words]\n",
    "        \n",
    "        return lemmatized_doc_stopw #[self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='utf-8', analyzer='word',\n",
    "                                stop_words=[\"'s\", \"fx\"], ngram_range = (1,1), min_df = 2).fit(df_norm2.text)\n",
    "print(\"fit vectorizer with lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit vectorizer without lemmatization done in 0.700s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tf_vectorizer = CountVectorizer(encoding='utf-8', analyzer='word', stop_words='english',\n",
    "                                ngram_range = (1,1), min_df = 2, token_pattern = '[a-zA-Z]{2,}').fit(newsgroups_train.data)\n",
    "print(\"fit vectorizer without lemmatization done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='[a-zA-Z]{2,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_feature_names() returns vocabulary\n",
    "n_features = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19012"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aa',\n",
       "  'aaa',\n",
       "  'aardvark',\n",
       "  'aaron',\n",
       "  'aas',\n",
       "  'ab',\n",
       "  'abandon',\n",
       "  'abandoned',\n",
       "  'abbott',\n",
       "  'abbreviation'],\n",
       " ['zoomed',\n",
       "  'zooming',\n",
       "  'zorro',\n",
       "  'zou',\n",
       "  'zpixmap',\n",
       "  'zterm',\n",
       "  'zubov',\n",
       "  'zupancic',\n",
       "  'zupcic',\n",
       "  'zyxel']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf_vectorizer.get_feature_names()[:10], tf_vectorizer.get_feature_names()[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hmmm. I seem to recall that the attraction of solid state record-\\nplayers and radios in the 1960s wasn't better performance but lower\\nper-unit cost than vacuum-tube systems.\\n\\n\\tMind you, my father was a vacuum-tube fan in the 60s (Switched\\nto solid-state in the mid-seventies and then abruptly died; no doubt\\nthere's a lesson in that) and his account could have been biased.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_docs = tf_vectorizer.transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4744x19012 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 259893 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf_vectorizer = TfidfTransformer(sublinear_tf=False, use_idf = True).fit(tf_docs)\n",
    "tfidf_docs = tfidf_vectorizer.transform(tf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.decomposition.online_lda.LatentDirichletAllocation"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=4744 and n_features=19012...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.516s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_docs, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf_docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "lssu gl cpu maria dma mask stormed dominating highlighting thirty belonged numlock mod wrist ik sgi rex indiana datahand cache\n",
      "Topic #1:\n",
      "like use just know does god people don time think problem drive edu new work good way window want thanks\n",
      "Topic #2:\n",
      "game year team games like season don think good win right just play players got didn did won hockey league\n",
      "Topic #3:\n",
      "food msg health disease cause patients diet lib medical diseases blood eat libxmu xmu foods day brain common effect doctor\n",
      "Topic #4:\n",
      "entry output com pts file edu period la buf entries pp rules power printf van oname build eof pt vs\n",
      "Topic #5:\n",
      "pitt gordon banks skepticism patients soon geb intellect jxp chastity dsl shameful cadre surrender candida pain edu yeast medical treatment\n",
      "Topic #6:\n",
      "space nasa launch orbit research center data earth shuttle satellite april lunar national moon year mission university information years science\n",
      "Topic #7:\n",
      "church catholic pope holy orthodox son schism authority mydisplay divine churches doctrine canon dpy bishop tomb gc archbishop creed catholics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf_feature_names is vocabulary\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 19012)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row# of lda.components_ is topic#\n",
    "# columns are the topic's unnormalized word weights (for all vocabulary words), in the sequence of the vocabulary word order\n",
    "per_topic_distr_LDA = lda.components_\n",
    "per_topic_distr_LDA.shape\n",
    "#per_topic_distr_LDA.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TOPIC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax example:\n",
    "# >>> kkk\n",
    "# array([[1, 2, 3],\n",
    "#       [0, 4, 2]])\n",
    "# >>> np.argmax(kkk, 0)\n",
    "# array([0, 1, 0])\n",
    "# >>> np.argmax(kkk, 1)\n",
    "# array([2, 1])\n",
    "#\n",
    "# this will select the topic with the most word weight for each word in the vocabulary\n",
    "# after this, we can easily lookup the best topic of each vocabulary word by \n",
    "# most_p_topic[word_voca_index] -> word's topic (0 -7 in this case) \n",
    "most_p_topic = np.argmax(per_topic_distr_LDA, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_topic_distr_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19012,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_p_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_and_topic = zip(tf_feature_names, most_p_topic)\n",
    "# word2topic_dict = {word : 'topic_' + np.array_str(topic) for word, topic in word_and_topic}\n",
    "word2topic_dict = {word : 'topic_{}'.format(topic) for word, topic in word_and_topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positively', 'topic_1'),\n",
       " ('bottle', 'topic_6'),\n",
       " ('carumba', 'topic_4'),\n",
       " ('computer', 'topic_1'),\n",
       " ('cookiemonster', 'topic_4')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the top 5 words and their belonged topics\n",
    "from itertools import islice\n",
    "list(islice(word2topic_dict.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    text_list = text.split()\n",
    "    normalized_text = [x.lower() for x in text_list]\n",
    "    # Define an empty list\n",
    "    nostopwords_text = []\n",
    "    # Scan the words\n",
    "    for word in normalized_text:\n",
    "        # Determine if the word is contained in the stop words list\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            # If the word is not contained I append it\n",
    "            nostopwords_text.append(word)\n",
    "    tokenized_text = [word for word in nostopwords_text if re.search('[a-zA-Z]{2,}', word)]\n",
    "            \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_doc_to_topic(tokenized_text, prefix, doc_id_number, word2topic_dict):\n",
    "    doc_to_topic_list = [prefix + '_' + str(doc_id_number)]\n",
    "    # print('adding doc_to_topic header element {}'.format(doc_to_topic_list[0]))\n",
    "\n",
    "    for word in tokenized_text:\n",
    "        if word in word2topic_dict.keys():\n",
    "            doc_to_topic_list.append(word2topic_dict[word])\n",
    "        # else:\n",
    "        #    print('{} not found in word2topic_dict.keys'.format(word))\n",
    "\n",
    "    return doc_to_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.deprecated.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, docs_list, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.docs_list = docs_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            words_doc=tokenizer(doc)\n",
    "            tags_doc = map_doc_to_topic(words_doc, idx, word2topic_dict)\n",
    "            yield LabeledSentence(words = words_doc,\n",
    "                                                 tags = tags_doc)\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(models.doc2vec.LabeledSentence)\n",
    "        return models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence_training(object):\n",
    "    def __init__(self, sources, word2topic_dict):\n",
    "        self.labels_list = word2topic_dict\n",
    "        self.sources = sources\n",
    "        flipped = {}\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        print('len of sources is {}'.format(len(self.sources)))\n",
    "        for source, prefix in self.sources.items():\n",
    "            print(source)\n",
    "            newsgroups_train_cat = fetch_20newsgroups(subset='train',\n",
    "                                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=[source])\n",
    "            # print('len of newsgroups_train_cat is {}'.format(len(newsgroups_train_cat)))\n",
    "            # (Pdb) newsgroups_train_cat.keys() -> \n",
    "            # dict_keys(['data', 'filenames', 'target', 'description', 'DESCR', 'target_names'])\n",
    "            # import pdb; pdb.set_trace()\n",
    "            for idx, doc in enumerate(newsgroups_train_cat.data):\n",
    "                words_doc=tokenizer(doc)\n",
    "                tags_doc = map_doc_to_topic(words_doc, prefix, idx, word2topic_dict)\n",
    "                yield LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc)\n",
    "                \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        print('len of sources is {}'.format(len(self.sources)))\n",
    "        for source, prefix in self.sources.items():\n",
    "            newsgroups_train_cat = fetch_20newsgroups(subset='train',\n",
    "                                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                                      categories=[source])\n",
    "            print('len of newsgroups_train_cat is {}'.format(len(newsgroups_train_cat)))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # (Pdb) type(newsgroups_train_cat) -> <class 'sklearn.utils.Bunch'>\n",
    "            # (Pdb) type(newsgroups_train_cat.data) -> <class 'list'>\n",
    "            # (Pdb) len(newsgroups_train_cat.data) -> 593\n",
    "            # (Pdb) newsgroups_train_cat.data[0] -> document 1 strings, with newlines inside\n",
    "            # (Pdb) newsgroups_train_cat.data[1] -> document 2 strings, with newlines inside\n",
    "            # (Pdb) newsgroups_train_cat.target.shape -> (593,)\n",
    "            # (Pdb) newsgroups_train_cat.target.max() -> 0\n",
    "            for idx, doc in enumerate(newsgroups_train_cat.data):\n",
    "                words_doc=tokenizer(doc)\n",
    "                tags_doc = map_doc_to_topic(words_doc, prefix, idx, word2topic_dict)\n",
    "                self.sentences.append(LabeledSentence(words = words_doc,\n",
    "                                                     tags = tags_doc))\n",
    "        return self.sentences\n",
    "            \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.sys.ibm.pc.hardware': 'comp_sys_ibm_pc_hardware',\n",
       " 'comp.sys.mac.hardware': 'comp_sys_mac_hardware',\n",
       " 'comp.windows.x': 'comp_windows_x',\n",
       " 'rec.sport.baseball': 'rec_sport_baseball',\n",
       " 'rec.sport.hockey': 'rec_sport_hockey',\n",
       " 'sci.med': 'sci_med',\n",
       " 'sci.space': 'sci_space',\n",
       " 'soc.religion.christian': 'soc_religion_christian'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positively', 'topic_1'),\n",
       " ('bottle', 'topic_6'),\n",
       " ('carumba', 'topic_4'),\n",
       " ('computer', 'topic_1'),\n",
       " ('cookiemonster', 'topic_4')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(word2topic_dict.items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all input news group documents\n",
    "#    For all sentences in that document\n",
    "#        Generate gensim.models.deprecated.doc2vec.LabeledSentence\n",
    "#            (words, tags with group name and word's topics)\n",
    "it = LabeledLineSentence_training(categories_source, word2topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quote notes about LabeledSentence and TaggedDocument\n",
    "1. LabeledSentence is an older, deprecated name for the same simple object-type to encapsulate a text-example that is now called TaggedDocument. \n",
    "2. Any objects that have words and tags properties, each a list, will do.\n",
    "    - words is always a list of strings\n",
    "    - tags can be a mix of integers and strings, but in the common and most-efficient case, is just a list with a single id integer, starting at 0.)\n",
    "\n",
    "#### [Info about how to use Gensim doc2vec](https://medium.com/@mishra.thedeepak/doc2vec-in-a-simple-way-fa80bfe81104)\n",
    "1. In this example it uses filename and doc label\n",
    "2. And after the training it can print the vector of the file using its name\n",
    "\n",
    "    ```\n",
    "    docvec = d2v_model.docvecs[‘1.txt’] #if string tag used in training\n",
    "    print docvec\n",
    "    ```\n",
    "3. Or to get most similar document with similarity scores using document-index\n",
    "\n",
    "    ```\n",
    "    similar_doc = d2v_model.docvecs.most_similar(14) \n",
    "    print similar_doc\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sources is 8\n",
      "rec.sport.hockey\n",
      "<class 'gensim.models.deprecated.doc2vec.LabeledSentence'>\n",
      "LabeledSentence(['looks', 'like', 'edmonton', 'oilers', 'just', 'decided', 'european', 'vacation', 'spring', 'ranford', 'tugnutt', 'benning', 'manson', 'smith', 'buchberger', 'corson', 'playing', 'canada', 'podein', 'weight', 'playing', 'kravchuk', 'playing', 'russiansi', 'know', 'nagging', 'injuries', 'late', 'season', 'podein', 'interesting', 'casebecause', 'eligible', 'play', 'cape', 'breton', 'ahl', 'playoffs', 'like', 'kovalev', 'zubov', 'anderssonobviously', 'sather', 'pocklington', 'total', 'scrooges', 'makes', 'becertainly', 'case', 'theyve', 'massively', 'outclassed', 'paramount', 'new', 'york', 'rangers'], ['rec_sport_hockey_0', 'topic_2', 'topic_1', 'topic_2', 'topic_2', 'topic_1', 'topic_1', 'topic_6', 'topic_1', 'topic_2', 'topic_4', 'topic_4', 'topic_2', 'topic_4', 'topic_2', 'topic_4', 'topic_4', 'topic_2', 'topic_2', 'topic_2', 'topic_1', 'topic_2', 'topic_4', 'topic_2', 'topic_1', 'topic_2', 'topic_2', 'topic_2', 'topic_2', 'topic_2', 'topic_1', 'topic_2', 'topic_2', 'topic_4', 'topic_4', 'topic_2', 'topic_2', 'topic_1', 'topic_4', 'topic_4', 'topic_2', 'topic_2', 'topic_2', 'topic_1', 'topic_1', 'topic_1', 'topic_2', 'topic_2', 'topic_1', 'topic_2', 'topic_2'])\n",
      "51 56\n",
      "['rec_sport_hockey_0', 'topic_2', 'topic_1', 'topic_2', 'topic_2', 'topic_1', 'topic_1', 'topic_6', 'topic_1', 'topic_2'] ['looks', 'like', 'edmonton', 'oilers', 'just', 'decided', 'european', 'vacation', 'spring', 'ranford']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "# print the first new group's item #1\n",
    "inspect_item = next(iter(it))\n",
    "print(type(inspect_item))\n",
    "print(inspect_item)\n",
    "print(len(inspect_item.tags), len(inspect_item.words))\n",
    "print(inspect_item.tags[:10], inspect_item.words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/gensim/models/doc2vec.py:359: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sources is 8\n",
      "len of newsgroups_train_cat is 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n",
      "len of newsgroups_train_cat is 6\n"
     ]
    }
   ],
   "source": [
    "# type(models.Doc2Vec)\n",
    "model = models.Doc2Vec(size=100, window=10, min_count=4, dm=1, dbow_words=1,\n",
    "                              workers=50, alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "model.build_vocab(it.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:05<00:00, 18.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train(it.sentences_perm(), total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname =  os.getcwd() # Prints the working directory\n",
    "fname = fname + '/topic2vec_20NG_2_ndoc' + str(n_docs) + 'n_topic' + str(n_topics) + '.model'\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results\n",
    "A quick info about [how to use gensim doc2vec model to query words by label vector or vice versa](https://github.com/RaRe-Technologies/gensim/issues/1397)\n",
    "\n",
    "1. search words using word\n",
    "\n",
    "    ```\n",
    "    model.most_similar('word')\n",
    "    # only similar words were returned but not labels\n",
    "    ```\n",
    "2. search label by label\n",
    "    - use model.docvecs.most_similar to search for similar labels using labels\n",
    "3. search words by label\n",
    "\n",
    "    ```\n",
    "    model.docvecs['label']\n",
    "    model.similar_by_vector(label_vec)\n",
    "    # only similar words were returned\n",
    "    ```\n",
    "4. search labels by word\n",
    "\n",
    "    ```\n",
    "    word_vec = model['word']\n",
    "    model.docvecs.most_similar([word_vec])\n",
    "    # returns similar labels\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model back\n",
    "d2v_model = models.doc2vec.Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4752 [('sci_med_480', Doctag(offset=4060, word_count=19, doc_count=1)), ('comp_windows_x_490', Doctag(offset=1098, word_count=26, doc_count=1)), ('comp_sys_mac_hardware_455', Doctag(offset=4629, word_count=16, doc_count=1)), ('comp_sys_mac_hardware_487', Doctag(offset=4661, word_count=7, doc_count=1)), ('sci_space_236', Doctag(offset=2036, word_count=3192, doc_count=1))]\n"
     ]
    }
   ],
   "source": [
    "paragraphs_tag = d2v_model.docvecs.doctags\n",
    "type(paragraphs_tag)\n",
    "print(len(paragraphs_tag), list(islice(paragraphs_tag.items(),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4752, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragraphs_vector = d2v_model.docvecs.doctag_syn0\n",
    "ragraphs_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rec_sport_baseball_389', 0.35365036129951477),\n",
       " ('rec_sport_baseball_378', 0.34690025448799133),\n",
       " ('comp_sys_ibm_pc_hardware_563', 0.3393045663833618),\n",
       " ('sci_med_524', 0.3304837942123413),\n",
       " ('comp_sys_mac_hardware_518', 0.32284557819366455),\n",
       " ('sci_med_125', 0.3165658116340637),\n",
       " ('rec_sport_hockey_94', 0.28935739398002625),\n",
       " ('comp_sys_mac_hardware_292', 0.289227157831192),\n",
       " ('rec_sport_hockey_203', 0.285552978515625),\n",
       " ('rec_sport_baseball_449', 0.28499162197113037)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.most_similar(positive = ['sci_space_96'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lords', 0.39834630489349365),\n",
       " ('doesnt', 0.3908179998397827),\n",
       " ('darling', 0.3805291950702667),\n",
       " ('ahola', 0.35309669375419617),\n",
       " ('worry', 0.3530368208885193),\n",
       " ('destroyed', 0.3495197892189026),\n",
       " ('tale', 0.33816206455230713),\n",
       " ('cpus', 0.33736613392829895),\n",
       " ('lzone', 0.325408935546875),\n",
       " ('ears', 0.3231354355812073)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vec = d2v_model.docvecs['sci_space_96']\n",
    "d2v_model.wv.similar_by_vector(label_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('years', 0.8583284020423889),\n",
       " ('game', 0.8574833869934082),\n",
       " ('foremost', 0.7862314581871033),\n",
       " ('wouldnt', 0.7857553958892822),\n",
       " ('healing', 0.7753252983093262),\n",
       " ('important', 0.7587778568267822),\n",
       " ('mailing', 0.758521556854248),\n",
       " ('members', 0.7551657557487488),\n",
       " ('frequently', 0.7538352012634277),\n",
       " ('previous', 0.7529152631759644)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_vec = d2v_model.docvecs['topic_0']\n",
    "d2v_model.wv.similar_by_vector(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sci_space_96', 0.3805291950702667),\n",
       " ('rec_sport_hockey_236', 0.34010958671569824),\n",
       " ('rec_sport_baseball_592', 0.31272220611572266),\n",
       " ('soc_religion_christian_301', 0.29557496309280396),\n",
       " ('comp_windows_x_3', 0.29294127225875854),\n",
       " ('rec_sport_hockey_252', 0.2872934341430664),\n",
       " ('soc_religion_christian_265', 0.28440430760383606),\n",
       " ('sci_space_211', 0.2841663360595703),\n",
       " ('comp_sys_mac_hardware_86', 0.2825673818588257),\n",
       " ('soc_religion_christian_149', 0.28180262446403503)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec = model['darling']\n",
    "model.docvecs.most_similar([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644150725372538"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.n_similarity(['topic_0', 'topic_2'], ['topic_3', 'topic_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3563553612509076"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.similarity('topic_0', 'topic_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
